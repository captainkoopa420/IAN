import os
import json
import logging
import random
import torch
import torch.nn as nn
import transformers
import numpy as np
from typing import List, Dict, Optional, Tuple
from nltk.sentiment import SentimentIntensityAnalyzer
import nltk
from nltk import download
import schedule
import time
import openai
import networkx as nx
import spacy
import wikipedia
import gym
from gym import spaces
from stable_baselines3 import DQN, PPO
from tkinter import *
from tkinter import ttk
import sqlite3
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import math
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor

# Set up logging
logging.basicConfig(filename='ian_thoughts.log', level=logging.INFO)

# Download required NLTK data
download('vader_lexicon')
download('punkt')

# Knowledge Base
class KnowledgeBase:
    def __init__(self):
        self.graph = nx.DiGraph()
        self.update_file = "knowledge_base_update.json"
        self.load_knowledge_base_update()
        self.conn = sqlite3.connect('knowledge_base.db')
        self.cursor = self.conn.cursor()
        self.create_knowledge_table()

    def create_knowledge_table(self):
        self.cursor.execute('''CREATE TABLE IF NOT EXISTS knowledge
                               (id INTEGER PRIMARY KEY,
                                concept TEXT NOT NULL,
                                description TEXT NOT NULL)''')
        self.conn.commit()

    def update_knowledge_base(self, new_entries):
        for entry in new_entries:
            self.graph.add_node(entry['id'], text=entry['text'])
            self.cursor.execute("INSERT INTO knowledge (concept, description) VALUES (?, ?)",
                                (entry['id'], entry['text']))
            for neighbor_id in entry.get('neighbors', []):
                self.graph.add_edge(entry['id'], neighbor_id)
        self.conn.commit()

    def load_knowledge_base_update(self):
        if os.path.exists(self.update_file):
            with open(self.update_file, 'r') as f:
                updates = json.load(f)
                self.update_knowledge_base(updates)

    def retrieve_information(self, query: str) -> List[Dict[str, str]]:
        keywords = query.split()
        relevant_nodes = [node for node in self.graph.nodes if any(keyword.lower() in node.lower() for keyword in keywords)]
        relevant_information = [{"id": node, "description": self.graph.nodes[node]["text"]} for node in relevant_nodes]
        for node in relevant_nodes:
            for related_node in self.graph.successors(node):
                relevant_information.append({"id": related_node, "description": self.graph.nodes[related_node]["text"]})
        return relevant_information

    def explore_new_topic(self) -> str:
        centrality_scores = nx.degree_centrality(self.graph)
        sorted_nodes = sorted(centrality_scores, key=centrality_scores.get)
        if sorted_nodes:
            return sorted_nodes[0]
        else:
            return "No new topic found"

# Sentiment Analyzer
class SentimentAnalyzer:
    def __init__(self):
        self.nltk_sentiment_analyzer = SentimentIntensityAnalyzer()
        self.model = transformers.AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
        self.tokenizer = transformers.AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment")
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-5)

    def analyze_sentiment(self, text: str) -> str:
        input_ids = self.tokenizer.encode(text, return_tensors="pt")
        sentiment_scores = self.model(input_ids)[0]
        compound_score = sentiment_scores.sum().item()
        return "positive" if compound_score > 0 else "negative"

    def analyze_sentiment_with_context(self, text: str, dialogue_history: List[str]) -> str:
        contextual_text = " ".join(dialogue_history) + " " + text
        input_ids = self.tokenizer.encode(contextual_text, return_tensors="pt")
        sentiment_scores = self.model(input_ids)[0]
        compound_score = sentiment_scores.sum().item()
        return "positive" if compound_score > 0 else "negative"

    def train(self, data):
        for text, label in data.items():
            encoded_text = self.tokenizer.encode(text, return_tensors="pt")
            labels = torch.tensor([1 if label == "positive" else 0]).unsqueeze(0)
            self.model.train()
            outputs = self.model(encoded_text, labels=labels)
            loss = outputs.loss
            loss.backward()
            self.optimizer.step()
            self.optimizer.zero_grad()

# DQN Agent
class EnhancedDQNAgent:
    def __init__(self, state_size, action_size, seed, gamma=0.99, tau=0.001, learning_rate=0.0005):
        self.state_size = state_size
        self.action_size = action_size
        self.seed = random.seed(seed)
        self.gamma = gamma
        self.tau = tau
        self.learning_rate = learning_rate
        self.qnetwork_local = self.build_enhanced_model()
        self.qnetwork_target = self.build_enhanced_model()
        self.optimizer = torch.optim.Adam(self.qnetwork_local.parameters(), lr=self.learning_rate)

    def build_enhanced_model(self):
        model = nn.Sequential(
            nn.Linear(self.state_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, self.action_size)
        )
        return model

    def get_state_representation(self, dialogue_state, knowledge_graph, user_profile):
        dialogue_vector = self.dialogue_state_to_vector(dialogue_state)
        kg_vector = self.knowledge_graph_to_vector(knowledge_graph)
        user_vector = self.user_profile_to_vector(user_profile)
        return np.concatenate([dialogue_vector, kg_vector, user_vector])

    def dialogue_state_to_vector(self, dialogue_state):
        vector = []
        vector.append(len(dialogue_state.get('conversation_history', [])))
        vector.append(self.goal_to_numeric(dialogue_state.get('user_goal', '')))
        vector.append(self.action_to_numeric(dialogue_state.get('user_action', '')))
        vector.extend(self.belief_state_to_vector(dialogue_state.get('belief_state', {})))
        return np.array(vector)

    def goal_to_numeric(self, goal):
        goals = ["learning", "information seeking", "task completion", "assistance", "conversation", "general interaction"]
        return goals.index(goal) if goal in goals else len(goals)

    def action_to_numeric(self, action):
        actions = ["ask_question", "provide_information"]
        return actions.index(action) if action in actions else len(actions)

    def belief_state_to_vector(self, belief_state):
        return [
            len(belief_state.get('entities', [])),
            belief_state.get('confidence', 0.0)
        ]

    def knowledge_graph_to_vector(self, knowledge_graph):
        return [
            knowledge_graph.number_of_nodes(),
            knowledge_graph.number_of_edges(),
            nx.density(knowledge_graph)
        ]

    def user_profile_to_vector(self, user_profile):
        vector = []
        vector.append(len(user_profile.get('preferences', {})))
        vector.append(len(user_profile.get('interests', [])))
        vector.append(sum(user_profile.get('knowledge_level', {}).values()) / max(len(user_profile.get('knowledge_level', {})), 1))
        vector.append(self.communication_style_to_numeric(user_profile.get('communication_style', '')))
        return np.array(vector)

    def communication_style_to_numeric(self, style):
        styles = ["formal", "casual", "technical", "simple"]
        return styles.index(style) if style in styles else len(styles)

    def calculate_reward(self, response, user_feedback, sentiment):
        relevance_score = self.calculate_relevance(response)
        informativeness_score = self.calculate_informativeness(response)
        sentiment_score = self.sentiment_to_score(sentiment)
        return (relevance_score + informativeness_score + sentiment_score) / 3

    def calculate_relevance(self, response):
        words = response.split()
        return min(1.0, len(words) / 50) * (len(set(words)) / len(words))

    def calculate_informativeness(self, response):
        doc = self.nlp(response)
        return min(1.0, len(doc.ents) / 5)

    def sentiment_to_score(self, sentiment):
        return 1.0 if sentiment == "positive" else 0.0

    def act(self, state, epsilon=0.1):
        state = torch.FloatTensor(state)
        self.qnetwork_local.eval()
        with torch.no_grad():
            action_values = self.qnetwork_local(state)
        self.qnetwork_local.train()

        if random.random() > epsilon:
            return np.argmax(action_values.cpu().data.numpy())
        else:
            return random.choice(np.arange(self.action_size))

    def step(self, state, action, reward, next_state, done):
        target = reward + (self.gamma * np.max(self.qnetwork_target(next_state).cpu().data.numpy()) * (1 - done))
        current = self.qnetwork_local(state)[action]
        loss = (current - target)**2
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        self.soft_update(self.qnetwork_local, self.qnetwork_target)

    def soft_update(self, local_model, target_model):
        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

    def save(self, filename):
        torch.save(self.qnetwork_local.state_dict(), filename)

    def load(self, filename):
        self.qnetwork_local.load_state_dict(torch.load(filename))

# Dialogue Manager
class DialogueManager:
    def __init__(self):
        self.state = None
        self.user_history = []
        self.conversation_goal = None
        self.conversation_history = []
        self.dialogue_state = {}
        self.policy = DialoguePolicy()
        self.nlp = spacy.load("en_core_web_md")

    def update_state(self, user_input: str):
        self.conversation_history.append(("user", user_input))
        self.user_history.append(user_input)
        self.dialogue_state = self.track_dialogue_state(user_input)
        self.state = {
            "turn": len(self.user_history),
            "last_input": user_input,
            "user_history": self.user_history,
            "conversation_goal": self.conversation_goal,
            "conversation_history": self.conversation_history
        }

    def track_dialogue_state(self, user_input: str) -> Dict:
        state = {
            "user_goal": self.extract_user_goal(user_input),
            "system_action": self.state.get("last_system_action", None),
            "user_action": self.extract_user_action(user_input),
            "belief_state": self.update_belief_state(user_input)
        }
        return state

    def extract_user_goal(self, user_input: str) -> str:
        goals = {
            "learn": "learning",
            "know": "information seeking",
            "find": "information seeking",
            "do": "task completion",
            "help": "assistance",
            "chat": "conversation"
        }

        for keyword, goal in goals.items():
            if keyword in user_input.lower():
                return goal
        return "general interaction"

    def extract_user_action(self, user_input: str) -> str:
        intent_and_entities = self.extract_intent_and_entities(user_input)
        return intent_and_entities['intent']

    def extract_intent_and_entities(self, user_input: str) -> Dict[str, str]:
        doc = self.nlp(user_input)
        intent = doc.cats.get("ASK") and "ask_question" or "provide_information"
        entities = [{"text": ent.text, "label": ent.label_} for ent in doc.ents]
        return {"intent": intent, "entities": entities}

    def update_belief_state(self, user_input: str) -> Dict:
        intent_and_entities = self.extract_intent_and_entities(user_input)
        belief_state = {
            "intent": intent_and_entities['intent'],
            "entities": intent_and_entities['entities'],
            "confidence": 0.8
        }
        return belief_state

# Dialogue Policy
class DialoguePolicy:
    def __init__(self):
        self.model = DQN("MlpPolicy", DialogueEnv())

    def train(self, num_episodes: int):
        self.model.learn(total_timesteps=num_episodes)

    def get_action(self, state: Dict) -> str:
        action, _ = self.model.predict(self.state_to_vector(state))
        return self.action_to_response(action)

    def state_to_vector(self, state: Dict) -> np.array:
        vector = []
        vector.append(self.goal_to_numeric(state.get('user_goal', '')))
        vector.append(self.action_to_numeric(state.get('user_action', '')))
        vector.extend(self.belief_state_to_vector(state.get('belief_state', {})))
        return np.array(vector)

    def goal_to_numeric(self, goal):
        goals = ["learning", "information seeking", "task completion", "assistance", "conversation", "general interaction"]
        return goals.index(goal) if goal in goals else len(goals)

    def action_to_numeric(self, action):
        actions = ["ask_question", "provide_information"]
        return actions.index(action) if action in actions else len(actions)

    def belief_state_to_vector(self, belief_state):
        return [
            len(belief_state.get('entities', [])),
            belief_state.get('confidence', 0.0)
        ]

    def action_to_response(self, action: int) -> str:
        actions = [
            "I'm sorry, could you please provide more information?",
            "Based on what you've told me, I think...",
            "That's an interesting point. Have you considered...",
            "Let me look that up for you.",
            "I'm not sure about that. Could you rephrase your question?"
        ]
        return actions[action] if action < len(actions) else "I'm not sure how to respond to that."

# Dialogue Environment
class DialogueEnv(gym.Env):
    def __init__(self):
        super().__init__()
        self.action_space = spaces.Discrete(5)
        self.observation_space = spaces.Box(low=0, high=1, shape=(10,), dtype=np.float32)
        self.state = None
        self.max_turns = 10
        self.current_turn = 0

    def step(self, action):
        self.current_turn += 1
        user_response = "This is a simulated user response."
        self.state = self.update_state(action, user_response)
        reward = self.calculate_reward(action, user_response)
        done = self.current_turn >= self.max_turns
        return self.state, reward, done, {}

    def reset(self):
        self.current_turn = 0
        self.state = np.zeros(10)
        return self.state

    def update_state(self, action, user_response):
        new_state = np.zeros(10)
        new_state[action] = 1
        new_state[-1] = self.current_turn / self.max_turns
        return new_state

    def calculate_reward(self, action, user_response):
        if "thank you" in user_response.lower() or "that's helpful" in user_response.lower():
            return 1
        elif "don't understand" in user_response.lower() or "that's not what I asked" in user_response.lower():
            return -1
        else:
            return 0

# Enhanced Natural Language Generation
class EnhancedNLG:
    def __init__(self, api_key):
        self.api_key = api_key
        openai.api_key = self.api_key

    @lru_cache(maxsize=100)
    def cached_nlg_response(self, prompt):
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "system", "content": prompt}]
        )
        return response['choices'][0]['message']['content']

    def generate_response(self, prompt, user_profile, dialogue_history):
        enhanced_prompt = self.create_enhanced_prompt(prompt, user_profile, dialogue_history)
        return self.cached_nlg_response(enhanced_prompt)

    def create_enhanced_prompt(self, prompt, user_profile, dialogue_history):
        return f"""
        User Profile: {user_profile}
        Dialogue History: {dialogue_history}
        Current Query: {prompt}
        Please provide a response that is tailored to the user's profile and context of the conversation.
        """

# User Model
class UserModel:
    def __init__(self):
        self.users = {}

    def update_user_model(self, user_id, interaction_data):
        if user_id not in self.users:
            self.users[user_id] = {"preferences": {}, "interests": [], "knowledge_level": {}, "communication_style": ""}

        self.update_preferences(user_id, interaction_data)
        self.update_interests(user_id, interaction_data)
        self.update_knowledge_level(user_id, interaction_data)
        self.update_communication_style(user_id, interaction_data)

    def update_preferences(self, user_id, interaction_data):
        user = self.users[user_id]
        input_text = interaction_data['input'].lower()

        preference_keywords = {
            "like": 1,
            "love": 2,
            "enjoy": 1,
            "prefer": 1,
            "dislike": -1,
            "hate": -2
        }

        for keyword, score in preference_keywords.items():
            if keyword in input_text:
                words = input_text.split()
                idx = words.index(keyword)
                if idx + 1 < len(words):
                    preference = words[idx + 1]
                    user['preferences'][preference] = user['preferences'].get(preference, 0) + score

    def update_interests(self, user_id, interaction_data):
        user = self.users[user_id]
        input_text = interaction_data['input'].lower()

        potential_interests = [word for word in input_text.split() if len(word) > 5]

        for interest in potential_interests:
            if interest not in user['interests']:
                user['interests'].append(interest)

    def update_knowledge_level(self, user_id, interaction_data):
        user = self.users[user_id]
        input_text = interaction_data['input'].lower()

        knowledge_indicators = {
            "expert": 2,
            "familiar": 1,
            "know": 1,
            "understand": 1,
            "novice": -1,
            "beginner": -1,
            "unfamiliar": -1
        }

        for indicator, score in knowledge_indicators.items():
            if indicator in input_text:
                words = input_text.split()
                idx = words.index(indicator)
                if idx + 1 < len(words):
                    topic = words[idx + 1]
                    user['knowledge_level'][topic] = user['knowledge_level'].get(topic, 0) + score

    def update_communication_style(self, user_id, interaction_data):
        user = self.users[user_id]
        input_text = interaction_data['input']

        if len(input_text.split()) > 20:
            style = "detailed"
        elif any(char in input_text for char in '!?'):
            style = "expressive"
        elif input_text.isupper():
            style = "assertive"
        else:
            style = "neutral"

        user['communication_style'] = style

    def get_user_profile(self, user_id):
        return self.users.get(user_id, {})

# Continual Learning Manager
class ContinualLearningManager:
    def __init__(self, knowledge_base, intent_classifier, sentiment_analyzer, dqn_agent):
        self.knowledge_base = knowledge_base
        self.intent_classifier = intent_classifier
        self.sentiment_analyzer = sentiment_analyzer
        self.dqn_agent = dqn_agent

    def update_knowledge_base(self, new_information):
        self.knowledge_base.update_knowledge_base(new_information)

    def retrain_models(self, new_data):
        self.retrain_intent_classifier(new_data)
        self.retrain_sentiment_analyzer(new_data)
        self.retrain_dqn_agent(new_data)

    def retrain_intent_classifier(self, new_data):
        if self.intent_classifier:
            self.intent_classifier.add_training_data(new_data['input'], new_data['intent'])
            self.intent_classifier.train()

    def retrain_sentiment_analyzer(self, new_data):
        self.sentiment_analyzer.train({new_data['input']: new_data['sentiment']})

    def retrain_dqn_agent(self, new_data):
        state = self.dqn_agent.get_state_representation(new_data['dialogue_state'], new_data['knowledge_graph'], new_data['user_profile'])
        next_state = self.dqn_agent.get_state_representation(new_data['next_dialogue_state'], new_data['knowledge_graph'], new_data['user_profile'])
        self.dqn_agent.step(state, new_data['action'], new_data['reward'], next_state, new_data['done'])

# CAWF Integration
class CognitiveAmplitudeWaveFunction:
    def __init__(self, num_facets: int):
        self.num_facets = num_facets
        self.amplitudes = np.zeros(num_facets)
        self.wave_numbers = np.zeros(num_facets)
        self.angular_frequencies = np.zeros(num_facets)
        self.phase_shifts = np.zeros(num_facets)

    def add_facet(self, amplitude: float, wave_number: float, angular_frequency: float, phase_shift: float):
        index = np.random.randint(self.num_facets)
        self.amplitudes[index] = amplitude
        self.wave_numbers[index] = wave_number
        self.angular_frequencies[index] = angular_frequency
        self.phase_shifts[index] = phase_shift

    def calculate_cawf(self, x: np.ndarray, t: float) -> np.ndarray:
        result = np.zeros_like(x, dtype=np.complex128)
        for i in range(self.num_facets):
            result += (self.amplitudes[i] * (np.cos(2 * np.pi * self.wave_numbers[i] * x - self.angular_frequencies[i] * t + self.phase_shifts[i]) +
                                             1j * np.sin(2 * np.pi * self.wave_numbers[i] * x - self.angular_frequencies[i] * t + self.phase_shifts[i])))
        return result

    def visualize_cawf(self, x_range: Tuple[float, float], t: float):
        x = np.linspace(x_range[0], x_range[1], 1000)
        y = self.calculate_cawf(x, t)
        y_magnitude = np.abs(y)**2

        plt.figure(figsize=(10, 6))
        plt.plot(x, y_magnitude, label='|Œ®(x, t)|¬≤')
        plt.xlabel('x')
        plt.ylabel('Amplitude')
        plt.title('Cognitive Amplitude Wave Function')
        plt.legend()
        plt.grid(True)
        plt.show()

class CAWFIntegration:
    def __init__(self, num_facets: int):
        self.cawf = CognitiveAmplitudeWaveFunction(num_facets)
        self.facet_mapping = {
            'knowledge': 0,
            'confidence': 1,
            'emotion': 2,
            'goals': 3,
            'attention': 4
        }
        self.history = []

    def update_facets(self, state: Dict):
        for facet, index in self.facet_mapping.items():
            amplitude = self.calculate_amplitude(state, facet)
            wave_number = self.calculate_wave_number(state, facet)
            angular_frequency = self.calculate_angular_frequency(state, facet)
            phase_shift = self.calculate_phase_shift(state, facet)
            self.cawf.add_facet(amplitude, wave_number, angular_frequency, phase_shift)

        self.history.append(self.cawf.calculate_cawf(np.linspace(0, 2, 1000), 0.5))

    def calculate_amplitude(self, state: Dict, facet: str) -> float:
        if facet == 'knowledge':
            return len(state.get('knowledge_base', {}).get('graph', {}).nodes()) / 1000
        elif facet == 'confidence':
            return state.get('dialogue_state', {}).get('belief_state', {}).get('confidence', 0.5)
        elif facet == 'emotion':
            sentiment = state.get('sentiment', 'neutral')
            return 0.5 if sentiment == 'neutral' else (1.0 if sentiment == 'positive' else 0.0)
        elif facet == 'goals':
            return len(state.get('dialogue_state', {}).get('user_goal', '')) / 20
        elif facet == 'attention':
            return len(state.get('dialogue_history', [])) / 50
        return 0.5

    def calculate_wave_number(self, state: Dict, facet: str) -> float:
        return 1.0  # Simplified for now, can be adjusted based on specific requirements

    def calculate_angular_frequency(self, state: Dict, facet: str) -> float:
        return 2 * np.pi  # Simplified for now, can be adjusted based on specific requirements

    def calculate_phase_shift(self, state: Dict, facet: str) -> float:
        return 0.0  # Simplified for now, can be adjusted based on specific requirements

    def get_cawf_state(self, x: np.ndarray, t: float) -> np.ndarray:
        return self.cawf.calculate_cawf(x, t)

    def visualize_cawf(self, x_range: Tuple[float, float], t: float):
        self.cawf.visualize_cawf(x_range, t)

    def explain_cawf(self):
        explanation = {
            "facets": {
                "knowledge": "Represents the breadth and depth of the AI's knowledge base.",
                "confidence": "Reflects the AI's certainty in its responses and decisions.",
                "emotion": "Captures the sentiment the AI perceives in user interactions.",
                "goals": "Represents the AI's understanding of user goals.",
                "attention": "Measures how actively the AI is participating in the dialogue."
            }
        }
        return explanation

class EnhancedIAN:
    def __init__(self):
        self.dialogue_manager = DialogueManager()
        self.dqn_agent = EnhancedDQNAgent(state_size=200, action_size=10, seed=0)
        self.nlg = EnhancedNLG(api_key="your-openai-api-key")
        self.user_model = UserModel()
        self.continual_learning_manager = ContinualLearningManager(
            knowledge_base=KnowledgeBase(),
            intent_classifier=None,
            sentiment_analyzer=SentimentAnalyzer(),
            dqn_agent=self.dqn_agent
        )
        self.cawf_integration = CAWFIntegration(num_facets=5)

    def process_input(self, user_id, user_input):
        if not isinstance(user_id, str) or not isinstance(user_input, str):
            raise ValueError("Invalid input. User ID and input message must be strings.")

        response = self._async_process_input(user_id, user_input)
        return response.result()

    def _async_process_input(self, user_id, user_input):
        with ThreadPoolExecutor() as executor:
            future = executor.submit(self._process_input, user_id, user_input)
            return future

    def _process_input(self, user_id, user_input):
        self.dialogue_manager.update_state(user_input)
        user_profile = self.user_model.get_user_profile(user_id)
        state = self.dqn_agent.get_state_representation(
            self.dialogue_manager.dialogue_state,
            self.continual_learning_manager.knowledge_base.graph,
            user_profile
        )
        action = self.dqn_agent.act(state)
        response = self.nlg.generate_response(user_input, user_profile, self.dialogue_manager.conversation_history)
        self.user_model.update_user_model(user_id, {"input": user_input, "response": response})
        self.continual_learning_manager.update_knowledge_base({"input": user_input, "response": response})
        self.continual_learning_manager.retrain_models({"input": user_input, "response": response})

        state = {
            'knowledge_base': self.continual_learning_manager.knowledge_base,
            'dialogue_state': self.dialogue_manager.dialogue_state,
            'sentiment': self.continual_learning_manager.sentiment_analyzer.analyze_sentiment(user_input),
            'dialogue_history': self.dialogue_manager.conversation_history
        }
        self.cawf_integration.update_facets(state)

        return response

    def get_cawf_state(self):
        x = np.linspace(0, 2, 1000)
        t = 0.5
        return self.cawf_integration.get_cawf_state(x, t)

    def visualize_cawf(self):
        self.cawf_integration.visualize_cawf((0, 2), 0.5)

    def explain_cawf(self):
        return self.cawf_integration.explain_cawf()

class IANGUI:
    def __init__(self, master, ian_system):
        self.master = master
        self.ian = ian_system
        master.title("IAN - Intelligent Assistant Network")
        master.geometry("800x600")
        master.configure(bg='#A8D0E6')

        self.create_widgets()
        self.queue = queue.Queue()
        self.update_gui()

    def create_widgets(self):
        top_bar = Frame(self.master, bg='#3A4F7A', height=40)
        top_bar.pack(fill=X)
        Label(top_bar, text="IAN", font=("Courier", 14, "bold"), bg='#3A4F7A', fg='white').pack(pady=5)

        content = Frame(self.master, bg='#F8F8F8')
        content.pack(expand=True, fill=BOTH, padx=20, pady=20)

        self.chat_display = Text(content, wrap=WORD, width=50, height=20, font=("Courier", 10, "bold"))
        self.chat_display.grid(row=0, column=0, columnspan=2, padx=10, pady=10)
        self.chat_display.configure(bg='white', relief=RIDGE, borderwidth=5)

        self.user_input = Entry(content, width=40, font=("Courier", 10, "bold"))
        self.user_input.grid(row=1, column=0, padx=10, pady=10)

        send_button = Button(content, text="SEND", command=self.send_message, bg='#FF6B6B', fg='white', font=("Courier", 10, "bold"))
        send_button.grid(row=1, column=1, padx=10, pady=10)

        feedback_frame = Frame(content, bg='#F8F8F8')
        feedback_frame.grid(row=2, column=0, columnspan=2, pady=10)

        self.thumbs_up_btn = Button(feedback_frame, text="üëç", command=self.thumbs_up, font=("Courier", 10, "bold"))
        self.thumbs_up_btn.pack(side=LEFT, padx=5)

        self.thumbs_down_btn = Button(feedback_frame, text="üëé", command=self.thumbs_down, font=("Courier", 10, "bold"))
        self.thumbs_down_btn.pack(side=LEFT, padx=5)

        self.status_display = Label(content, text="Ready for your next command, Trainer!", font=("Courier", 10, "bold"), bg='white', relief=RIDGE, borderwidth=5)
        self.status_display.grid(row=3, column=0, columnspan=2, padx=10, pady=10, sticky='ew')

        bottom_menu = Frame(self.master, bg='#F8F8F8')
        bottom_menu.pack(side=BOTTOM, fill=X, padx=20, pady=20)

        menu_buttons = [
            ("CHAT", '#FF6B6B', self.show_chat),
            ("LEARN", '#4ECDC4', self.show_learn),
            ("EXPLORE", '#45B7D1', self.show_explore),
            ("HELP", '#FFA07A', self.show_help)
        ]

        for text, color, command in menu_buttons:
            Button(bottom_menu, text=text, bg=color, fg='white', font=("Courier", 10, "bold"), command=command).pack(side=LEFT, expand=True, padx=5)

        self.create_cawf_visualization()

    def send_message(self):
        user_message = self.user_input.get()
        self.chat_display.insert(END, f"You: {user_message}\n")
        self.user_input.delete(0, END)
        threading.Thread(target=self.process_message, args=(user_message,)).start()

    def process_message(self, user_message):
        response = self.ian.process_input("user123", user_message)
        self.queue.put(("chat", f"IAN: {response}\n"))
        self.queue.put(("status", "Response generated! What do you think, Trainer?"))
        self.queue.put(("feedback", "enable"))

    def thumbs_up(self):
        self.provide_feedback(True)

    def thumbs_down(self):
        self.provide_feedback(False)

    def provide_feedback(self, is_positive):
        self.ian.process_feedback("user123", is_positive)
        feedback_text = "Thanks for the positive feedback!" if is_positive else "Sorry to hear that. We'll try to improve!"
        self.queue.put(("status", feedback_text))
        self.queue.put(("feedback", "disable"))

    def show_chat(self):
        self.queue.put(("status", "Chat mode activated!"))

    def show_learn(self):
        self.queue.put(("status", "Learn mode activated! What would you like to know?"))

    def show_explore(self):
        self.queue.put(("status", "Explore mode activated! Let's discover new topics!"))

    def show_help(self):
        self.queue.put(("status", "Help mode activated! How can I assist you?"))

    def create_cawf_visualization(self):
        self.cawf_canvas = Canvas(self.master, width=400, height=200)
        self.cawf_canvas.pack(side=RIGHT, padx=10, pady=10)
        self.update_cawf_visualization()

    def update_cawf_visualization(self):
        self.cawf_canvas.delete("all")
        cawf_state = self.ian.get_cawf_state()
        x = np.linspace(0, 2, 1000)
        y = np.abs(cawf_state)**2
        y_normalized = (y - np.min(y)) / (np.max(y) - np.min(y)) * 180

        for i in range(len(x) - 1):
            x1, y1 = i * 400 / len(x), 200 - y_normalized[i]
            x2, y2 = (i + 1) * 400 / len(x), 200 - y_normalized[i + 1]
            self.cawf_canvas.create_line(x1, y1, x2, y2, fill="blue")

        self.master.after(1000, self.update_cawf_visualization)

    def update_gui(self):
        try:
            while True:
                message = self.queue.get_nowait()
                if message[0] == "chat":
                    self.chat_display.insert(END, message[1])
                    self.chat_display.see(END)
                elif message[0] == "status":
                    self.status_display.config(text=message[1])
                elif message[0] == "feedback":
                    if message[1] == "enable":
                        self.thumbs_up_btn.config(state=NORMAL)
                        self.thumbs_down_btn.config(state=NORMAL)
                    else:
                        self.thumbs_up_btn.config(state=DISABLED)
                        self.thumbs_down_btn.config(state=DISABLED)
        except queue.Empty:
            pass
        finally:
            self.master.after(100, self.update_gui)

if __name__ == "__main__":
    root = Tk()
    ian_system = EnhancedIAN()
    gui = IANGUI(root, ian_system)
    root.mainloop()
